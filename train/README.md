# AI3603-Billiards 训练环境

本目录 (`train/`) 包含了用于训练 `NewAgent` 神经网络模型的脚本和工具。

## 1. 训练核心思想

本项目采用 **模仿学习 (Imitation Learning)** 与 **辅助价值评估** 的策略。

*   **教师模型 (Teacher)**: `BasicAgentPro` (MCTS Agent)。它计算量大、决策慢，但基于大量模拟，决策质量极高。
*   **学生模型 (Student)**: `NewAgent`。它需要决策快，因此我们训练一个神经网络来“直觉”地判断局面的好坏。
*   **价值网络 (Value Network)**: 一个 MLP (多层感知机) 神经网络，输入当前台面球的状态（位置、归属），输出当前玩家的预期胜率。

## 2. 训练流程

运行 `train.py` 将执行以下步骤：

1.  **数据收集 (Self-Play / Versus)**:
    *   让 `NewAgent` (带有当前模型) 与 `BasicAgentPro` (MCTS 强手) 进行多局对战（默认 50 局）。
    *   记录每一杆击球前的 **局面状态 (State)** 以及最终的 **比赛胜负 (Winner)**。
    *   如果是 `NewAgent` 赢了，则它经历的所有局面被标记为高价值 (+1)；输了则标记为低价值 (-1)。

2.  **神经网络训练**:
    *   使用收集到的 `(State, Value)` 数据对 `BilliardValueNet` 进行监督学习训练。
    *   优化目标是最小化预测价值与实际胜负结果之间的均方误差 (MSE)。

3.  **模型更新**:
    *   训练完成后，模型保存为 `../eval/billiard_value_net.pth`。
    *   `NewAgent` 在下一轮测试或训练中会自动加载这个新模型，利用更准确的局面评估来辅助决策。

## 3. 目录文件

*   **`train.py`**: 主训练脚本。包含数据收集循环、模型定义、训练循环。
*   *(注意：训练依赖 `eval/` 目录下的环境代码，脚本会自动处理路径引用)*

## 4. 如何运行训练

在 `train` 目录下运行：

```bash
python train.py
```

**参数调整 (在 `train.py` 中修改):**

*   `collect_data(n_games=50)`: 设置数据收集的对战局数。局数越多，数据越丰富，训练效果越好，但耗时越长。
*   `epochs=10`: 神经网络训练的轮数。

## 5. 硬件建议

*   由于 `BasicAgentPro` 使用 MCTS 进行大量物理模拟，数据收集阶段对 CPU 计算能力要求较高。
*   神经网络训练阶段计算量较小，普通 CPU 即可胜任，有 GPU 加速更佳（脚本会自动检测 CUDA）。
